{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a292d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 85 pages from PDFs\n",
      "‚úÖ Created 252 text chunks\n",
      "‚úÖ Vector store created\n",
      "‚úÖ Retrieved 12 relevant sections\n",
      "‚úÖ Methodology comparison completed\n",
      "‚úÖ Comparison matrix built\n",
      "‚úÖ Conflicts identified\n",
      "‚úÖ Research gaps identified\n",
      "\n",
      "================ FINAL OUTPUT ================\n",
      "\n",
      "üìä COMPARISON MATRIX:\n",
      "\n",
      "Here is a comparison matrix based on the extracted information from the three research papers:\n",
      "\n",
      "| Papers                          | Method                                                                 | Dataset                                                                 | Strengths                                                                                          | Limitations                                                                                     |\n",
      "|---------------------------------|------------------------------------------------------------------------|------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------|\n",
      "| **Paper 1: Summarization Quality Evaluation** | Human evaluation of summarization quality using trained graders. Graders assess summaries based on specific guidelines and dimensions. | A diverse set of evaluation datasets simulating real user inputs, including single and stacked documents of varying content types and lengths. | - Utilizes highly-trained human graders to ensure quality and consistency in evaluation. <br> - Emphasizes a diverse range of inputs to reflect real-world scenarios. | - Potential biases in human grading. <br> - Limited scalability due to reliance on human evaluators. |\n",
      "| **Paper 2: AFM Model Evaluation and Training** | Hybrid data strategy combining human-annotated and synthetic data for model training. Reinforcement learning from human feedback (RLHF) and rejection sampling fine-tuning algorithms. | A mixture of human-annotated demonstration datasets and synthetic data generated through self-instruct methods. | - Focus on data quality and diversity, leading to improved model performance. <br> - Iterative improvement through human feedback enhances model capabilities. | - Complexity in managing and integrating both human and synthetic data. <br> - Dependence on human feedback may introduce variability in evaluation. |\n",
      "| **Paper 3: Safety Evaluation in AI Models** | Red teaming projects involving internal and external crowds to evaluate model safety. Comprehensive safety evaluation sets filtered for challenging prompts. | Safety evaluation sets designed to be clean yet comprehensive, focusing on prompts that yield varied responses. | - Emphasis on responsible AI development and user safety. <br> - Structured approach to red teaming ensures thorough evaluation. | - Potential ethical concerns regarding the use of crowds for sensitive evaluations. <br> - Challenges in ensuring the comprehensiveness of safety evaluation sets. |\n",
      "\n",
      "This matrix provides a clear and concise comparison of the methodologies, datasets, strengths, and limitations of each paper.\n",
      "\n",
      "‚ö†Ô∏è CONFLICTS:\n",
      "\n",
      "- - **Methodological Conflicts:**\n",
      "-   - **Paper 1** relies solely on human evaluation for summarization quality, while **Paper 2** employs a hybrid approach that combines human-annotated and synthetic data, indicating a divergence in evaluation methods.\n",
      "-   - **Paper 3** focuses on safety evaluation through red teaming, which is distinct from the summarization quality and model training evaluations in Papers 1 and 2.\n",
      "- \n",
      "- - **Dataset Differences:**\n",
      "-   - **Paper 1** uses diverse evaluation datasets simulating real user inputs, whereas **Paper 2** combines human-annotated datasets with synthetic data, suggesting a conflict in the type of data used for evaluation and training.\n",
      "-   - **Paper 3** emphasizes safety evaluation sets that are clean yet comprehensive, which may not align with the diverse content types highlighted in Paper 1.\n",
      "- \n",
      "- - **Strengths vs. Limitations:**\n",
      "-   - **Paper 1**'s strength in using trained human graders is countered by the limitation of potential biases, which may not be a concern in **Paper 2** where human feedback is integrated into model training but introduces variability.\n",
      "-   - **Paper 3** emphasizes responsible AI development, but its reliance on crowds for evaluations raises ethical concerns, contrasting with the structured human evaluation in Paper 1.\n",
      "- \n",
      "- - **Focus Areas:**\n",
      "-   - **Paper 1** and **Paper 2** focus on summarization and model performance, while **Paper 3** prioritizes safety, indicating a potential conflict in research priorities and objectives across the papers. \n",
      "- \n",
      "- - **Evaluation Consistency:**\n",
      "-   - The reliance on human evaluators in **Paper 1** and **Paper 2** may lead to inconsistencies in evaluation outcomes, especially when compared to the structured red teaming approach in **Paper 3**.\n",
      "\n",
      "üîç RESEARCH GAPS:\n",
      "\n",
      "- ### Research Gaps\n",
      "- - Lack of comprehensive studies integrating multiple variables in the comparison matrix.\n",
      "- - Insufficient exploration of the contextual factors influencing conflicting results.\n",
      "- - Limited understanding of the long-term impacts of the identified variables.\n",
      "- \n",
      "- ### Missing Datasets\n",
      "- - Absence of longitudinal datasets that track changes over time related to the variables in the comparison matrix.\n",
      "- - Lack of diverse demographic datasets that could provide insights into how different populations are affected.\n",
      "- - Missing datasets that include qualitative data to complement quantitative findings.\n",
      "- \n",
      "- ### Underexplored Methods\n",
      "- - Need for advanced statistical techniques to reconcile conflicting results, such as meta-analysis or machine learning approaches.\n",
      "- - Exploration of mixed-methods research to provide a more holistic view of the issues at hand.\n",
      "- - Application of network analysis to understand the interrelationships between variables in the comparison matrix.\n",
      "- \n",
      "- ### Future Research Directions\n",
      "- - Investigate the underlying mechanisms that lead to conflicting results in existing studies.\n",
      "- - Develop standardized protocols for data collection and analysis to enhance comparability across studies.\n",
      "- - Encourage interdisciplinary collaboration to leverage diverse methodologies and perspectives.\n",
      "- - Focus on real-world applications of findings to assess their practical implications and effectiveness.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import TypedDict, List, Annotated\n",
    "import operator\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ---------------------------\n",
    "# ENV SETUP\n",
    "# ---------------------------\n",
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# ---------------------------\n",
    "# LANGCHAIN / LANGGRAPH\n",
    "# ---------------------------\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.messages import BaseMessage, AIMessage\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# ---------------------------\n",
    "# STATE DEFINITION\n",
    "# ---------------------------\n",
    "class ResearchAgentState(TypedDict):\n",
    "    folder_path: str\n",
    "    query: str\n",
    "\n",
    "    papers: list\n",
    "    chunks: list\n",
    "    vectorstore: FAISS | None\n",
    "    retrieved_docs: list\n",
    "\n",
    "    extracted_info: str\n",
    "    comparison_matrix: str\n",
    "    conflicts: List[str]\n",
    "    research_gaps: List[str]\n",
    "\n",
    "    messages: Annotated[List[BaseMessage], operator.add]\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# NODE 1: LOAD PDFs\n",
    "# ---------------------------\n",
    "def load_pdfs(state: ResearchAgentState):\n",
    "    papers = []\n",
    "    for file in os.listdir(state[\"folder_path\"]):\n",
    "        if file.endswith(\".pdf\"):\n",
    "            loader = PyPDFLoader(os.path.join(state[\"folder_path\"], file))\n",
    "            papers.extend(loader.load())\n",
    "\n",
    "    print(f\"‚úÖ Loaded {len(papers)} pages from PDFs\")\n",
    "    return {\"papers\": papers}\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# NODE 2: SPLIT DOCUMENTS\n",
    "# ---------------------------\n",
    "def split_documents(state: ResearchAgentState):\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1200,\n",
    "        chunk_overlap=200\n",
    "    )\n",
    "    chunks = splitter.split_documents(state[\"papers\"])\n",
    "    print(f\"‚úÖ Created {len(chunks)} text chunks\")\n",
    "    return {\"chunks\": chunks}\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# NODE 3: CREATE VECTOR STORE\n",
    "# ---------------------------\n",
    "def create_vectorstore(state: ResearchAgentState):\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "    vectorstore = FAISS.from_documents(state[\"chunks\"], embeddings)\n",
    "    print(\"‚úÖ Vector store created\")\n",
    "    return {\"vectorstore\": vectorstore}\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# NODE 4: RETRIEVE SECTIONS\n",
    "# ---------------------------\n",
    "def retrieve_relevant_sections(state: ResearchAgentState):\n",
    "    docs = state[\"vectorstore\"].similarity_search(\n",
    "        \"methodology experimental design dataset results findings\",\n",
    "        k=12\n",
    "    )\n",
    "    print(f\"‚úÖ Retrieved {len(docs)} relevant sections\")\n",
    "    return {\"retrieved_docs\": docs}\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# NODE 5: COMPARE METHODOLOGIES\n",
    "# ---------------------------\n",
    "def compare_methodologies(state: ResearchAgentState):\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "    context = \"\\n\\n\".join(\n",
    "        doc.page_content[:1500] for doc in state[\"retrieved_docs\"]\n",
    "    )\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Analyze the methodologies used in the following research papers.\n",
    "\n",
    "Extract for each paper:\n",
    "- Methodology\n",
    "- Dataset\n",
    "- Model / Algorithm\n",
    "- Evaluation Metrics\n",
    "- Key Strengths\n",
    "- Limitations\n",
    "\n",
    "Return structured text.\n",
    "\n",
    "Papers:\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "    response = llm.invoke(prompt)\n",
    "    print(\"‚úÖ Methodology comparison completed\")\n",
    "    return {\"extracted_info\": response.content}\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# NODE 6: BUILD COMPARISON MATRIX\n",
    "# ---------------------------\n",
    "def build_comparison_matrix(state: ResearchAgentState):\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Using the extracted information below, build a comparison matrix.\n",
    "\n",
    "Rows = Papers  \n",
    "Columns = Method, Dataset, Strengths, Limitations\n",
    "\n",
    "Extracted Info:\n",
    "{state[\"extracted_info\"]}\n",
    "\"\"\"\n",
    "\n",
    "    response = llm.invoke(prompt)\n",
    "    print(\"‚úÖ Comparison matrix built\")\n",
    "    return {\"comparison_matrix\": response.content}\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# NODE 7: IDENTIFY CONFLICTS\n",
    "# ---------------------------\n",
    "def identify_conflicts(state: ResearchAgentState):\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Identify conflicting or contradicting findings across the papers.\n",
    "\n",
    "Comparison Matrix:\n",
    "{state[\"comparison_matrix\"]}\n",
    "\n",
    "Return bullet points.\n",
    "\"\"\"\n",
    "\n",
    "    response = llm.invoke(prompt)\n",
    "    print(\"‚úÖ Conflicts identified\")\n",
    "    return {\"conflicts\": response.content.split(\"\\n\")}\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# NODE 8: FIND RESEARCH GAPS\n",
    "# ---------------------------\n",
    "def find_research_gaps(state: ResearchAgentState):\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Based on:\n",
    "- Comparison matrix\n",
    "- Conflicting results\n",
    "\n",
    "Identify:\n",
    "- Research gaps\n",
    "- Missing datasets\n",
    "- Underexplored methods\n",
    "- Future research directions\n",
    "\n",
    "Return bullet points.\n",
    "\"\"\"\n",
    "\n",
    "    response = llm.invoke(prompt)\n",
    "    print(\"‚úÖ Research gaps identified\")\n",
    "    return {\"research_gaps\": response.content.split(\"\\n\")}\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# BUILD LANGGRAPH\n",
    "# ---------------------------\n",
    "graph = StateGraph(ResearchAgentState)\n",
    "\n",
    "graph.add_node(\"load_pdfs\", load_pdfs)\n",
    "graph.add_node(\"split_documents\", split_documents)\n",
    "graph.add_node(\"create_vectorstore\", create_vectorstore)\n",
    "graph.add_node(\"retrieve_relevant_sections\", retrieve_relevant_sections)\n",
    "graph.add_node(\"compare_methodologies\", compare_methodologies)\n",
    "graph.add_node(\"build_comparison_matrix\", build_comparison_matrix)\n",
    "graph.add_node(\"identify_conflicts\", identify_conflicts)\n",
    "graph.add_node(\"find_research_gaps\", find_research_gaps)\n",
    "\n",
    "graph.set_entry_point(\"load_pdfs\")\n",
    "\n",
    "graph.add_edge(\"load_pdfs\", \"split_documents\")\n",
    "graph.add_edge(\"split_documents\", \"create_vectorstore\")\n",
    "graph.add_edge(\"create_vectorstore\", \"retrieve_relevant_sections\")\n",
    "graph.add_edge(\"retrieve_relevant_sections\", \"compare_methodologies\")\n",
    "graph.add_edge(\"compare_methodologies\", \"build_comparison_matrix\")\n",
    "graph.add_edge(\"build_comparison_matrix\", \"identify_conflicts\")\n",
    "graph.add_edge(\"identify_conflicts\", \"find_research_gaps\")\n",
    "graph.add_edge(\"find_research_gaps\", END)\n",
    "\n",
    "app = graph.compile()\n",
    "\n",
    "# ---------------------------\n",
    "# RUN\n",
    "# ---------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    initial_state: ResearchAgentState = {\n",
    "        \"folder_path\": \"papers\",\n",
    "        \"query\": \"Compare research methodologies\",\n",
    "        \"papers\": [],\n",
    "        \"chunks\": [],\n",
    "        \"vectorstore\": None,\n",
    "        \"retrieved_docs\": [],\n",
    "        \"extracted_info\": \"\",\n",
    "        \"comparison_matrix\": \"\",\n",
    "        \"conflicts\": [],\n",
    "        \"research_gaps\": [],\n",
    "        \"messages\": []\n",
    "    }\n",
    "\n",
    "    final_state = app.invoke(initial_state)\n",
    "\n",
    "    print(\"\\n================ FINAL OUTPUT ================\\n\")\n",
    "    print(\"üìä COMPARISON MATRIX:\\n\")\n",
    "    print(final_state[\"comparison_matrix\"])\n",
    "\n",
    "    print(\"\\n‚ö†Ô∏è CONFLICTS:\\n\")\n",
    "    for c in final_state[\"conflicts\"]:\n",
    "        print(\"-\", c)\n",
    "\n",
    "    print(\"\\nüîç RESEARCH GAPS:\\n\")\n",
    "    for g in final_state[\"research_gaps\"]:\n",
    "        print(\"-\", g)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qaagent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
